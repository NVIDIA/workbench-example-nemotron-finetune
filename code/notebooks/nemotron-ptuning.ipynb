{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b296604",
   "metadata": {},
   "source": [
    "# P-Tuning Nemotron-3 With A Custom Dataset\n",
    "Nemotron-3 is a robust, powerful family of Large Language Models that can provide compelling responses on a wide range of tasks. While the 8B parameter base model serves as a strong baseline for multiple downstream tasks, they can lack in domain-specific knowledge or proprietary or otherwise sensitive information. Fine-tuning is often used as a means to update a model for a specific task or tasks to better respond to domain-specific prompts. This notebook walks through preparing a dataset and p-tuning the base Nemotron-3 8B model from Hugging Face against the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5daa3fc",
   "metadata": {},
   "source": [
    "## Getting the model\n",
    "\n",
    "A `Nemotron-3-8B-Base-4k.nemo` file should be present in `models/nemotron-3-8b-base-4k`. If not, run the following cell to download the model in NeMo format; this can take <ins>a few minutes</ins> to complete! This `Nemotron-3-8B-Base-4k.nemo` file will be used for fine-tuning for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /project/code/scripts/init.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619219c",
   "metadata": {},
   "source": [
    "## Preparing The Dataset\n",
    "The dataset being used for fine-tuning needs to be converted to a .jsonl file and follow a specific format. In general, question and answer datasets are easiest to work with by providing context (if applicable), a question, and the expected answer, though different downstream tasks work as well.\n",
    "\n",
    "### Downloading the dataset\n",
    "This notebook will use the [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) - available on Hugging Face - as an example. However, this can replaced with other datasets available on Hugging Face as well. If using a dataset not available on Hugging Face, manually upload or download the dataset into `data` directory of the project and follow the steps below.\n",
    "\n",
    "As seen in the output, the dataset is currently in the form of a Hugging Face `DatasetDict` with 15,015 entries each including an `instruction`, `context`, `response`, and `category` field. For custom datasets, ensure the data has a similar structure of one unique item per row and all rows having the same fields. The fields do not need to match that of the Dolly dataset but should all match each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5006ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "\n",
    "dataset = load_dataset(\"aisquared/databricks-dolly-15k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5044b2",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "Some datasets may contain unnecessary fields. For the example with the Dolly dataset, we do not need the `closed_qa` field for the fine-tuned model and will remove those lines from the dataset. Additionally, the `instruction`, `response`, and `category` fields will be renamed to `question`, `answer`, and `taskname`, respectively. NeMo Toolkit requires p-tuning datasets to include `taskname` to specify which task an element belongs to in the case of multiple tasks. For this case, we will use `genqa` for the taskname for all items. In general, these fields are the most common to work with. Other field names can be used but the configuration step later in this notebook may need to be updated to reflect these custom fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_column(example):\n",
    "    example[\"taskname\"] = 'genqa'\n",
    "    return example\n",
    "\n",
    "dataset = dataset.filter(lambda example: example[\"category\"].startswith(\"closed_qa\"))\n",
    "dataset = dataset.rename_column(\"instruction\", \"question\")\n",
    "dataset = dataset.rename_column(\"response\", \"answer\")\n",
    "dataset = dataset.rename_column(\"category\", \"taskname\")\n",
    "dataset = dataset.map(set_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf9d42",
   "metadata": {},
   "source": [
    "After filtering the dataset above, we can see the first item in the dataset has a `question`, `context`, `answer`, and `taskname` field. Note that the following cell block might not work for custom datasets with different subsets other than `train`, `test`, and `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d74a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c92fc5",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test files\n",
    "\n",
    "The prompt learning dataset loader accepts a list of json/dictionary objects or a list of json file names where each json file contains a collection of json objects. Each json object must include the field taskname which is a string identifier for the task the data example corresponds to. They should also include one or more fields corresponding to different sections of the discrete text prompt. The input data looks like:\n",
    "```\n",
    "[\n",
    "    {\"taskname\": \"genqa\", \"context\": [CONTEXT_PARAGRAPH_TEXT1], \"question\": [QUESTION_TEXT1], \"answer\": [ANSWER_TEXT1]},\n",
    "    {\"taskname\": \"genqa\", \"context\": [CONTEXT_PARAGRAPH_TEXT2], \"question\": [QUESTION_TEXT2], \"answer\": [ANSWER_TEXT2]},\n",
    "]\n",
    "```\n",
    "\n",
    "To create the files, we need to split the dataset object between train and validation files by taking the first 90% of the object and putting it in a `*train.jsonl` file and the remainder in a `*val.jsonl` file. Note that the split percentage can be changed as well as items being randomly sampled from the dataset to fill the quota if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/project/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba912a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_test_split = 0.9\n",
    "\n",
    "with open(\"/project/data/dolly_train.jsonl\", 'w') as f:\n",
    "    for index, item in enumerate(dataset['train']):\n",
    "        if index < int(train_test_split*len(dataset['train'])):\n",
    "            f.write(json.dumps(item) + \"\\n\")  \n",
    "\n",
    "with open(\"/project/data/dolly_val.jsonl\", 'w') as f:\n",
    "    for index, item in enumerate(dataset['train']):\n",
    "        if index >= int(train_test_split*len(dataset['train'])):\n",
    "            f.write(json.dumps(item) + \"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea68d9",
   "metadata": {},
   "source": [
    "Let's take a look at a row in the training dataset file. Similar to the previous dataset output, this shows the fields of `question`, `context`, `answer`, and `taskname`. This format is used for both the `train` and `val` files. These files will be used as datasets for the p-tuning process. The dataset is now ready to be used for p-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 $DATA_DIR/dolly_train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe4d64",
   "metadata": {},
   "source": [
    "## Configuring the job\n",
    "With the dataset preparation finished, we need to update the default configuration for our fine-tuning job. The sample config file provided by NeMo is a good template to base our changes on. Let's load the file as an object that we can edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"/opt/NeMo/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79414d97",
   "metadata": {},
   "source": [
    "With the config loaded, we can override certain settings for our environment. Many of the default values shown here would work but some key points are called out below:\n",
    "\n",
    "* `config.trainer.precision=\"32\"` - This is the precision that will be used during p-tuning. The model might be more accurate with higher values but it also uses more memory than lower precisions. If the p-tuning process runs out of memory, try reducing the precision here.\n",
    "* `config.trainer.devices=1` - This is the number of devices that will be used. If running on a multi-GPU system, increase this number as appropriate.\n",
    "* `config.model.restore_from_path=\"/project/models/nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo\"` - This is the path to the converted `.nemo` checkpoint from the beginning of the notebook. If the path changed, update it here.\n",
    "* `config.model.data.train_ds.file_names` and `config.model.data.validation_ds.file_names` - If a different filename or path was used for the dataset files created earlier, specify the new values here.\n",
    "* `config.model.global_batch_size` - If using a higher GPU count or if additional GPU memory allows, this value can be increased for higher performance. Note that higher batch sizes use more GPU memory.\n",
    "* `config.model.data.train_ds.prompt_template` - If different field names were used during the dataset creation earlier, update them here with the intended field names. This is what NeMo Toolkit will look for in each dataset element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d34b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trainer.precision=\"32\"\n",
    "config.trainer.devices=1\n",
    "config.trainer.num_nodes=1\n",
    "config.trainer.max_epochs=3 \n",
    "config.trainer.max_steps=1000\n",
    "config.trainer.val_check_interval=50\n",
    "config.model.restore_from_path=\"/project/models/nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo\"\n",
    "config.model.peft.peft_scheme=\"ptuning\"\n",
    "config.model.data.train_ds.file_names=[\"/project/data/dolly_train.jsonl\"] \n",
    "config.model.data.validation_ds.file_names=[\"/project/data/dolly_val.jsonl\"]\n",
    "config.model.global_batch_size=2 \n",
    "config.model.micro_batch_size=1 \n",
    "config.model.optim.lr=0.0001\n",
    "config.model.data.train_ds.concat_sampling_probabilities=[1.0] \n",
    "config.model.data.train_ds.prompt_template=\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:{answer}\" \n",
    "config.model.peft.p_tuning.virtual_tokens=15 \n",
    "config.model.data.train_ds.label_key=\"answer\"\n",
    "config.model.data.train_ds.truncation_field=\"context\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c6c08",
   "metadata": {},
   "source": [
    "With the config settings updated, save it as a `.yaml` file that can be read by NeMo Toolkit during p-tuning and save it to the p-tuning configuration directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0936dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmegaConf.save can also accept a `str` or `pathlib.Path` instance:\n",
    "OmegaConf.save(config, \"nemotron-config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv nemotron-config.yaml /opt/NeMo/examples/nlp/language_modeling/tuning/conf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc1c95",
   "metadata": {},
   "source": [
    "## Launching the job\n",
    "With the model downloaded, the dataset prepped, and the config set, it is now time to launch the p-tuning job! The following block launches the job on the specified number of GPUs. Depending on the size of the dataset and the GPU used, this could take anywhere from a few minutes to several hours to finish. As the model is tuned, checkpoints will be saved in the `nemo_experiments` directory inside the container. These checkpoints contain prompt embeddings which are used to send inference requests with the p-tuned weights to deployed models so they respond as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    --config-name=nemotron-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bcbaa",
   "metadata": {},
   "source": [
    "### Configuring the evaluation script\n",
    "The configuration file for the evaluation job needs to be updated based on the provided template to reflect the changes for the experiment here. Load the template config and update some of the settings to match the local environment. Note the following settings may differ for custom datasets:\n",
    "* `config.model.data.test_ds.file_names` - List any prediction files that should be used to evaluate the model. In general, it is recommended to have this be different from the training and validation files used during p-tuning. For simplicities sake, we generate a single example here. You may add more if you wish. \n",
    "* `config.model.data.test_ds.names` - Change the name of the dataset used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our test file. Add your own examples!\n",
    "\n",
    "import json\n",
    "test = [{\"question\": \"Is the lawn mower product solar powered?\", \n",
    "         \"context\": \"The Auto Chef Master is a personal kitchen robot that effortlessly turns raw ingredients into gourmet meals with the precision of a Michelin-star chef. The Eco Lawn Mower is a solar powered high-tech lawn mower that provides an eco-friendly and efficient way to maintain your lawn.\", \n",
    "         \"answer\": \"Yes, the Eco Lawn Mower is solar powered.\", \n",
    "         \"taskname\": \"genqa\"}]\n",
    "        \n",
    "with open('/project/data/dolly_test.jsonl', 'w+') as outfile:\n",
    "    for entry in test:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the template config file\n",
    "config = OmegaConf.load(\"/opt/NeMo/examples/nlp/language_modeling/tuning/conf/megatron_gpt_generate_config.yaml\")\n",
    "\n",
    "# Override required settings\n",
    "config.peft_scheme=\"ptuning\"\n",
    "config.model.restore_from_path=\"/project/models/nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo\"\n",
    "config.model.peft.restore_from_path=\"/project/code/notebooks/nemo_experiments/megatron_gpt_peft_ptuning_tuning/checkpoints/megatron_gpt_peft_ptuning_tuning.nemo\"\n",
    "config.model.data.test_ds.file_names=[\"/project/data/dolly_test.jsonl\"]\n",
    "config.model.data.test_ds.names=\"dolly\"\n",
    "config.model.data.test_ds.global_batch_size=2\n",
    "config.model.data.test_ds.micro_batch_size=1\n",
    "config.model.data.test_ds.write_predictions_to_file=True\n",
    "config.model.data.test_ds.output_file_path_prefix=\"/project/code/notebooks/predictions\"\n",
    "config.model.data.test_ds.prompt_template=\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\"\n",
    "\n",
    "# Save the new config file\n",
    "OmegaConf.save(config, \"nemotron-eval-config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72147518",
   "metadata": {},
   "source": [
    "Once the config is saved, evaluation can be launched below. Depending on the size of the hardware and the number of inference examples, this may take a few minutes to complete. Results will be saved to `predictions_test_dolly_inputs_preds_labels.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv nemotron-eval-config.yaml /opt/NeMo/examples/nlp/language_modeling/tuning/conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py --config-name=nemotron-eval-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b695f7",
   "metadata": {},
   "source": [
    "Note depending on hardware and the number of examples used, the evaluation script may take a while to run since we are using a training container setting and not currently optimizing for inference. Once we are ready to serve the finetuned model for true deployment, we may then move the model to an optimized inference framework like Triton. \n",
    "\n",
    "After the evaluation script completes, view the results. Keep in mind the results you see may vary in quality: \n",
    "* The hyperparameters presented in this notebook are not optimal and only serve as examples. Could you be underfitting? Overfitting? These can be adjusted in the configs to improve performance! \n",
    "\n",
    "The point is fine tuning the out-of-the-box model to the general QA task is easy and straightforward with this workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 /project/code/notebooks/predictions_test_dolly_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc3c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
